{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üöÄ Large Language Model Fine-tuning Practical Tutorial"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üìò Welcome to Your LLM Fine-tuning Journey!\n\nIn this tutorial, we'll explore how to transform a powerful pre-trained model into your personalized AI assistant. Whether you're an AI beginner or an experienced developer, this guide will help you master the core techniques of model fine-tuning in the most intuitive way possible.\n\n---\n\n### üéØ Why Fine-tune Models?\n\nThink of pre-trained large language models as brilliant generalists‚Äîthey possess vast knowledge but may not understand your specific business needs. **Model fine-tuning** is the process of transforming this \"generalist\" into a \"domain expert\" for your field.\n\n#### Core Concepts Explained:\n- **Pre-trained Models**: Foundation models trained on massive text corpora with broad language understanding\n- **Fine-tuning Process**: Using your domain data to teach the model task-specific patterns and knowledge\n- **End Result**: A customized model that retains general capabilities while excelling at specific tasks\n\n---\n\n### üõ†Ô∏è Our Technology Stack\n\nWe've carefully selected an **efficient, user-friendly, and resource-conscious** technology stack:\n\n#### 1. **Unsloth Acceleration Engine** ‚ö°\n- Provides **2x training speedup**\n- Reduces **60% memory usage**\n- Enables large model training on consumer GPUs!\n\n#### 2. **LoRA Fine-tuning Technology** üéØ\n- Trains only **0.1% of parameters** while achieving near full fine-tuning performance\n- Post-training adapter files are just tens of MBs, easy to deploy and share\n- Supports flexible switching between multiple LoRA adapters\n\n#### 3. **4-bit Quantization** üíæ\n- Compresses model size to **1/4 of original**\n- Maintains performance while drastically reducing hardware requirements\n- Enables 70B models on 24GB consumer GPUs\n\n#### 4. **Base Model: Baidu ERNIE-4.5** ü§ñ\nWe're using Baidu's latest ERNIE-4.5 series as our foundation:\n- **0.3B version**: Perfect for quick experiments and learning (default in this tutorial)\n- **21B version**: Production-ready with high performance\n\n---\n\n### üìö Learning Roadmap\n\n```\nStep 1: Environment Setup (10 minutes)\n   ‚Üì\nStep 2: Model Loading & Configuration (5 minutes)\n   ‚Üì\nStep 3: Data Preparation & Formatting (10 minutes)\n   ‚Üì\nStep 4: Training Launch (20 minutes)\n   ‚Üì\nStep 5: Save & Deploy (5 minutes)\n```\n\nLet's begin this exciting AI journey! üéâ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üì¶ Step 1: Environment Setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîß Installing Required Libraries\n\nFirst, we need to install several key Python libraries that will provide all the functionality needed for model training.\n\n**Tip**: Installation takes about 2-3 minutes. Feel free to read ahead while waiting!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Install Core Libraries =====\n# Unsloth: Our training acceleration engine, installed from source for latest features\n!pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n\n# bitsandbytes: Provides model quantization to compress models to 4-bit or 8-bit\n# unsloth_zoo: Contains pre-configured models and toolsets\n!pip install bitsandbytes unsloth_zoo\n\n# transformers: Hugging Face's core library providing models and trainers\n!pip install -U transformers\n\nprint(\"‚úÖ Environment ready! All dependencies successfully installed.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ü§ñ Step 2: Load Model and Configure LoRA"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üí° Understanding LoRA: Making Large Model Fine-tuning Simple\n\nBefore diving into code, let's understand the magic of **LoRA (Low-Rank Adaptation)**:\n\n#### Traditional Fine-tuning vs LoRA Fine-tuning\n\n| Aspect | Traditional Full Fine-tuning | LoRA Fine-tuning |\n|--------|------------------------------|------------------|\n| Parameters Trained | 100% (billions) | 0.1-1% (millions) |\n| Memory Required | Extreme (>80GB) | Moderate (8-24GB) |\n| Training Time | Days | Hours |\n| Model File Size | Full model size | Adapter only ~100MB |\n| Performance | Best | Near-best |\n\n#### How LoRA Works\n\nThink of model knowledge updates like home renovation:\n- **Traditional method**: Tear down and rebuild the entire house (train all parameters)\n- **LoRA method**: Install \"adapters\" in key locations (train small new parameters)\n\nSpecifically, LoRA adds two small matrices (A and B) alongside the model's attention layers, training these small matrices to adjust model behavior:\n```\nOriginal output = W √ó input\nLoRA output = W √ó input + (B √ó A) √ó input\n              ‚Üëunchanged    ‚Üënew trainable part\n```\n\n#### Key Parameters Explained\n\n- **r (rank)**: LoRA matrix rank, controls adapter capacity\n  - r=8: Lightweight fine-tuning for simple tasks\n  - r=16: Balanced choice (used in this tutorial)\n  - r=32+: Complex tasks requiring more new knowledge\n  \n- **lora_alpha**: Scaling factor, typically set to 2√ó rank\n- **lora_dropout**: Prevents overfitting, usually 0.05-0.1\n- **target_modules**: Modules to apply LoRA to\n  - \"all-linear\": All linear layers (recommended)\n  - Can specify specific layers like [\"q_proj\", \"v_proj\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Step 1: Load Pre-trained Model =====\nfrom unsloth import FastModel\nimport torch\n\n# Set maximum sequence length (affects memory usage and training speed)\nMAX_LEN = 4096  # Adjust based on task: 2048 (chat), 4096 (balanced), 8192 (long text)\n\n# Model Selection Guide:\n# - baidu/ERNIE-4.5-0.3B-PT: Lightweight, perfect for learning and quick experiments (recommended for beginners)\n# - baidu/ERNIE-4.5-21B-A3B-PT: Production-grade, powerful but requires more resources\n\nprint(\"üîÑ Loading model, please wait...\")\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name=\"baidu/ERNIE-4.5-0.3B-PT\",  # Can switch to 21B version\n    # max_seq_length=MAX_LEN,               # Some versions require this\n    # load_in_4bit=False,                   # Set True for QLoRA if memory constrained\n    # load_in_8bit=False,                   # 8-bit quantization option\n    # full_finetuning=False,                # True for full fine-tuning (requires large memory)\n    trust_remote_code=True,                  # Required for ERNIE models\n)\nprint(\"‚úÖ Model loaded successfully!\")\n\n# ===== Step 2: Configure LoRA Adapters =====\nfrom unsloth import FastLanguageModel\n\nprint(\"üîß Configuring LoRA adapters...\")\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,                    # LoRA rank: 8 (lightweight), 16 (balanced), 32 (complex tasks)\n    lora_alpha=32,           # Scaling factor: typically 2√ó rank\n    lora_dropout=0.05,       # Dropout rate: prevents overfitting (0.05-0.1)\n    target_modules=\"all-linear\",  # Layers to apply LoRA: \"all-linear\" or specific layer list\n    use_rslora=True,         # Use improved RSLoRA (more stable)\n    # use_gradient_checkpointing=True,  # Saves memory but slows training\n)\n\n# Print model information\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\ntrainable_percent = 100 * trainable_params / all_params\n\nprint(f\"‚úÖ LoRA configuration complete!\")\nprint(f\"üìä Model Statistics:\")\nprint(f\"   - Total parameters: {all_params:,}\")\nprint(f\"   - Trainable parameters: {trainable_params:,}\")\nprint(f\"   - Trainable percentage: {trainable_percent:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä Step 3: Prepare Training Data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üìù The Importance of Data Formatting\n\nData is the \"textbook\" for model fine-tuning. Just as teaching requires appropriate materials, model training needs correctly formatted data.\n\n#### Why Data Format Matters\n\n1. **Model Understanding**: Models learned specific conversation formats during pre-training\n2. **Performance Impact**: Incorrect formatting causes models to \"misunderstand\" instructions\n3. **Consistency**: Maintaining format consistency helps models generalize better\n\n#### ERNIE's Conversation Format\n\nERNIE-4.5 uses a specific conversation template:\n```\n<|begin_of_sentence|>User: [user input]\nAssistant: [assistant response]<|end_of_sentence|>\n```\n\nThese special tokens help the model distinguish:\n- Start and end of conversations\n- Boundaries between user input and AI responses\n- Context in multi-turn dialogues\n\n#### Dataset Selection Recommendations\n\nThis tutorial uses Microsoft's Orca Math dataset as an example, but you can choose based on your needs:\n\n- **General Dialogue**: ShareGPT, Alpaca, etc.\n- **Math Reasoning**: GSM8K, MATH, etc.\n- **Code Generation**: CodeAlpaca, StarCoder, etc.\n- **Domain-Specific**: Medical, legal, financial specialized datasets\n- **Custom Data**: Your own business data (recommended)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Load and Format Dataset =====\nfrom datasets import load_dataset\n\nprint(\"üìö Loading dataset...\")\n# Using Microsoft's Orca math word problems dataset as example\n# split=\"train[:1%]\" means using only 1% of data for quick demonstration\n# For production, use complete dataset or your own data\nds = load_dataset(\"microsoft/orca-math-word-problems-200k\", split=\"train[:1%]\")\nprint(f\"‚úÖ Data loaded! Total samples: {len(ds)}\")\n\n# ===== Data Formatting Function =====\ndef format_chat(example):\n    \"\"\"\n    Convert raw data to ERNIE model's conversation format\n    \n    Input format:\n    - question: User's question\n    - answer: Expected response\n    \n    Output format:\n    <|begin_of_sentence|>User: [question]\n    Assistant: [answer]<|end_of_sentence|>\n    \"\"\"\n    # Build conversation message list\n    messages = [{\"role\": \"user\", \"content\": example[\"question\"]}]\n    \n    # Add assistant response if answer exists\n    if \"answer\" in example and example[\"answer\"]:\n        messages.append({\"role\": \"assistant\", \"content\": example[\"answer\"]})\n    \n    # Use tokenizer's template to automatically add special tokens\n    formatted_text = tokenizer.apply_chat_template(\n        messages, \n        tokenize=False,  # Don't tokenize, just format\n        add_generation_prompt=False  # No generation prompt needed for training\n    )\n    \n    return {\"text\": formatted_text}\n\n# ===== Apply Formatting =====\nprint(\"üîÑ Formatting data...\")\nds = ds.map(\n    format_chat,  # Apply formatting function\n    remove_columns=ds.column_names,  # Remove original columns, keep only \"text\"\n    desc=\"Formatting data\"  # Progress bar description\n)\n\n# ===== Validate Data Format =====\nprint(\"‚úÖ Data formatting complete!\")\nprint(\"\\nüìù Data Sample (first entry):\")\nprint(\"-\" * 50)\nprint(ds[0][\"text\"][:500])  # Show first 500 characters\nprint(\"-\" * 50)\nprint(f\"\\nüí° Tip: Verify the data contains correct special tokens!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîç Data Format Validation\n\nFrom the output above, we can see the data has been correctly formatted for the ERNIE model:\n- `<|begin_of_sentence|>` marks conversation start\n- `User:` and `Assistant:` clearly distinguish roles\n- `<|end_of_sentence|>` marks conversation end\n\nThis format ensures the model correctly understands task boundaries and role transitions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üöÇ Step 4: Configure and Launch Training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ‚öôÔ∏è Training Parameters Explained\n\nTraining configuration is key to successful fine-tuning. Let's understand each parameter's role and optimization tips:\n\n#### üéØ Batch Processing & Gradient Accumulation\n\n**Effective Batch Size = per_device_train_batch_size √ó gradient_accumulation_steps √ó Number of GPUs**\n\nIn our configuration:\n- `per_device_train_batch_size = 1`: Process 1 sample per forward pass\n- `gradient_accumulation_steps = 8`: Accumulate gradients for 8 steps before updating\n- **Effective Batch Size = 1 √ó 8 = 8**\n\nüí° **Optimization Tips**:\n- Sufficient memory: Increase `per_device_train_batch_size`\n- Limited memory: Reduce batch size, increase gradient accumulation\n- Target: Effective batch size between 8-32 typically works well\n\n#### üìà Learning Rate & Optimizer\n\n- `learning_rate = 1e-4`: Typical learning rate for LoRA fine-tuning\n  - Too high (>5e-4): Unstable training, oscillating loss\n  - Too low (<1e-5): Slow convergence, poor results\n  - Experience: 1e-4 to 2e-4 for LoRA, 1e-5 to 5e-5 for full fine-tuning\n\n- `optim = \"adamw_8bit\"`: 8-bit AdamW optimizer\n  - Saves 75% optimizer state memory\n  - Negligible performance loss\n  - Suitable for large models and long sequences\n\n- `lr_scheduler_type = \"linear\"`: Linear learning rate decay\n  - High learning rate early for rapid learning\n  - Low learning rate later for fine adjustments\n  - Alternatives: cosine (cosine decay), constant (fixed rate)\n\n#### üîÑ Training Epochs & Steps\n\n- `num_train_epochs = 1`: Number of training epochs\n  - Small datasets (<10k): 3-5 epochs\n  - Medium datasets (10k-100k): 1-3 epochs\n  - Large datasets (>100k): 1 epoch or use steps\n\n- Alternative using `max_steps`:\n  - Quick testing: 50-100 steps\n  - Normal training: 500-2000 steps\n  - Deep training: 5000+ steps\n\n#### üíæ Checkpoint Saving\n\n- `save_strategy = \"steps\"`: Save by steps\n- `save_steps = 200`: Save every 200 steps\n  - Enables resuming after interruption\n  - Allows selecting best checkpoint\n  - Recommendation: Set to 10-20% of total steps\n\n#### üöÄ Performance Optimization\n\n- `fp16 = True`: Use half-precision training\n  - 2x speed improvement\n  - 50% memory reduction\n  - Works on most GPUs (V100, T4, RTX, etc.)\n\n- `bf16 = False`: BFloat16 precision\n  - Better numerical stability\n  - Requires newer GPUs (A100, H100)\n  - Prefer bf16 if supported\n\n- `packing = True`: Sample packing\n  - Concatenates short samples for better GPU utilization\n  - Especially useful for varying-length datasets\n  - Can improve training speed 2-5x\n\n#### üìä Monitoring & Debugging\n\n- `logging_steps = 5`: Log every 5 steps\n  - Monitor loss decrease trend\n  - Quickly identify training issues\n  - Production: can set to 10-50\n\n- `report_to = \"none\"`: No external reporting\n  - Options: \"tensorboard\", \"wandb\"\n  - Useful for visualization and team collaboration\n\n### üéì Understanding Training Progress\n\nDuring training, you'll see output like:\n```\nStep | Training Loss | Validation Loss\n-----|---------------|----------------\n10   | 2.345         | -\n20   | 1.876         | -\n30   | 1.234         | -\n```\n\n**Normal behavior**:\n- Loss continuously decreases (from 2-3 down to 0.5-1.5)\n- Decrease rate gradually slows\n- No sudden jumps\n\n**Troubleshooting**:\n- Loss not decreasing ‚Üí Check data format, increase learning rate\n- Loss oscillating ‚Üí Reduce learning rate, increase gradient accumulation\n- Loss suddenly increases ‚Üí Possible overfitting, add dropout, reduce training steps"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Configure Training Parameters =====\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\nprint(\"‚öôÔ∏è Configuring training parameters...\")\n\n# Training arguments configuration\ntraining_args = TrainingArguments(\n    # ===== Batch Processing =====\n    per_device_train_batch_size=1,      # Batch size per device (adjust based on memory)\n    gradient_accumulation_steps=8,      # Gradient accumulation (effective batch = 1 √ó 8 = 8)\n    \n    # ===== Learning Rate =====\n    learning_rate=1e-4,                 # Initial learning rate (LoRA typically uses 1e-4 to 2e-4)\n    lr_scheduler_type=\"linear\",         # LR scheduler: \"linear\", \"cosine\", \"constant\"\n    warmup_steps=0,                     # Warmup steps (optional, typically 10% of total)\n    \n    # ===== Training Duration =====\n    num_train_epochs=1,                 # Number of epochs (or use max_steps instead)\n    # max_steps=100,                    # Maximum training steps (choose one with num_train_epochs)\n    \n    # ===== Optimizer =====\n    optim=\"adamw_8bit\",                 # 8-bit AdamW optimizer (saves memory)\n    weight_decay=0.01,                  # Weight decay (L2 regularization)\n    \n    # ===== Precision =====\n    fp16=True,                          # Use FP16 mixed precision (T4, V100, etc.)\n    bf16=False,                         # Use BF16 (A100, H100 newer GPUs)\n    \n    # ===== Saving & Logging =====\n    output_dir=\"outputs\",               # Output directory\n    save_strategy=\"steps\",              # Save strategy: \"steps\" or \"epoch\"\n    save_steps=200,                     # Save checkpoint every N steps\n    logging_steps=5,                    # Log every N steps\n    report_to=\"none\",                   # Reporting: \"tensorboard\", \"wandb\", or \"none\"\n    \n    # ===== Other Settings =====\n    remove_unused_columns=False,        # Keep all data columns\n    seed=42,                            # Random seed (for reproducibility)\n)\n\n# ===== Initialize Trainer =====\nprint(\"üöÄ Initializing SFT trainer...\")\n\ntrainer = SFTTrainer(\n    model=model,                        # Model with LoRA configured\n    tokenizer=tokenizer,                # Tokenizer\n    train_dataset=ds,                   # Training dataset\n    dataset_text_field=\"text\",          # Text field name in dataset\n    args=training_args,                 # Training arguments\n    max_seq_length=MAX_LEN,             # Maximum sequence length\n    packing=True,                       # Enable sample packing (improves GPU utilization)\n    use_cache=False,                    # Disable cache (required for LoRA training)\n)\n\nprint(\"‚úÖ Trainer configured!\")\nprint(f\"üìä Training Information:\")\nprint(f\"   - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   - Total training samples: {len(ds)}\")\nprint(f\"   - Estimated training steps: {len(ds) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n\n# ===== Start Training =====\nprint(\"\\nüèÉ Starting training... (this may take a few minutes)\")\nprint(\"üí° Tip: Watch for decreasing loss values\\n\")\n\n# Execute training\ntrainer_stats = trainer.train(\n    # resume_from_checkpoint=True,      # Resume from checkpoint if exists\n)\n\n# ===== Training Complete =====\nprint(\"\\nüéâ Training complete!\")\nprint(f\"üìä Training Statistics:\")\nprint(f\"   - Total training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\nprint(f\"   - Samples per second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\nprint(f\"   - Final loss: {trainer_stats.metrics['train_loss']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üíæ Step 5: Save the Fine-tuned Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Save Fine-tuned Model =====\nprint(\"üíæ Saving model...\")\n\n# Save path\nsave_path = \"ernie-4.5-0.3b-sft-merged\"\n\n# Execute save (merge LoRA weights into base model)\nmodel.save_pretrained_merged(\n    save_path,                          # Save path\n    tokenizer,                          # Also save tokenizer\n    save_method=\"merged_16bit\",         # Save method:\n                                       # - \"merged_16bit\": Merge and save as 16-bit (recommended)\n                                       # - \"merged_4bit\": 4-bit quantized save (smallest)\n                                       # - \"lora\": Save only LoRA adapter\n)\n\nprint(f\"‚úÖ Model saved successfully!\")\nprint(f\"üìÅ Save location: {save_path}/\")\nprint(f\"üì¶ File descriptions:\")\nprint(f\"   - config.json: Model configuration file\")\nprint(f\"   - model.safetensors: Model weights file\")\nprint(f\"   - tokenizer.json: Tokenizer file\")\nprint(f\"   - tokenizer_config.json: Tokenizer configuration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üéâ Congratulations! Training Complete\n\nYour fine-tuned model is saved at: `/content/ernie-4.5-0.3b-sft-merged`\n\n#### Save Options Explained\n\nWe used the `save_pretrained_merged` method, which offers several save options:\n\n1. **merged_16bit** (used in this tutorial)\n   - Merges LoRA weights back into original model\n   - Saves as 16-bit precision\n   - Moderate file size, fast inference\n   - Best for deployment\n\n2. **merged_4bit**\n   - 4-bit quantized save\n   - Smallest files (about 1/4 original)\n   - Best for resource-constrained environments\n\n3. **lora**\n   - Saves only LoRA adapter\n   - Extremely small files (typically <100MB)\n   - Requires original model to use\n   - Best for multi-task switching scenarios\n\n#### Next Steps: Using Your Model\n\n```python\n# Load your fine-tuned model for inference\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"ernie-4.5-0.3b-sft-merged\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\"ernie-4.5-0.3b-sft-merged\")\n\n# Generate responses with your model\nmessages = [{\"role\": \"user\", \"content\": \"Your question here\"}]\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=100)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Memory Cleanup (Optional) =====\n# Run this code if you need to free GPU memory for other tasks\n\n# import gc\n# import torch\n\n# # Delete model and trainer objects\n# del model, tokenizer, trainer\n# \n# # Force garbage collection\n# gc.collect()\n# \n# # Clear CUDA cache\n# if torch.cuda.is_available():\n#     torch.cuda.empty_cache()\n#     print(\"‚úÖ GPU memory cleared!\")"
  },
  {
   "cell_type": "markdown",
   "source": "## üìö Summary & Advanced Guide\n\n### üéØ Skills You've Mastered\n\nBy completing this tutorial, you've learned to:\n1. ‚úÖ Accelerate model training with Unsloth\n2. ‚úÖ Configure and apply LoRA fine-tuning\n3. ‚úÖ Prepare and format training data\n4. ‚úÖ Adjust training hyperparameters\n5. ‚úÖ Save and deploy fine-tuned models\n\n### üöÄ Advanced Optimization Tips\n\n#### 1. **Data Quality Optimization**\n- Ensure data diversity and balance\n- Clean low-quality samples\n- Add edge cases and challenging examples\n- Use data augmentation techniques\n\n#### 2. **Hyperparameter Tuning**\n- Use grid search or Bayesian optimization\n- Monitor validation set performance\n- Implement early stopping to prevent overfitting\n- Apply learning rate warmup strategies\n\n#### 3. **Multi-task Learning**\n- Train multiple LoRA adapters\n- Implement task routing mechanisms\n- Share base model to save resources\n\n#### 4. **Production Deployment**\n- Use ONNX or TensorRT for acceleration\n- Implement batch inference\n- Add caching mechanisms\n- Monitor model performance metrics\n\n### üîß Troubleshooting Guide\n\n| Issue | Possible Cause | Solution |\n|-------|----------------|----------|\n| OOM (Out of Memory) | Batch size too large/sequence too long | Reduce batch size/use gradient accumulation/enable quantization |\n| Loss not decreasing | Improper learning rate/data issues | Adjust learning rate/check data format |\n| Slow training | Unoptimized configuration | Enable fp16/use packing/upgrade Unsloth |\n| Poor results | Insufficient data/improper parameters | Increase data/adjust LoRA rank |\n\n### üìñ Recommended Learning Resources\n\n1. **Unsloth Documentation**: [github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)\n2. **LoRA Paper**: [arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)\n3. **Hugging Face Tutorials**: [huggingface.co/docs/peft](https://huggingface.co/docs/peft)\n4. **ERNIE Model Cards**: [huggingface.co/baidu](https://huggingface.co/baidu)\n\n### üí¨ Join the Community\n\n- Having issues? Open a GitHub Issue\n- Want to share experiences? Join our technical community\n- Have improvements? Submit a Pull Request\n\n---\n\n**Wishing you success on your AI fine-tuning journey!** üåü\n\n*If this tutorial helped you, please give our repository a ‚≠ê Star!*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}