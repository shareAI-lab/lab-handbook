{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pre-trained Large Language Models (LLMs) demonstrate exceptional general capabilities in natural language processing, efficiently adapting them to specific scenarios and maximizing their value has become a focal point for both industry and academia. This tutorial aims to provide developers and researchers with a complete, efficient, and resource-controlled fine-tuning practice solution.\n",
    "\n",
    "### I. Model Fine-tuning Overview\n",
    "\n",
    "Model fine-tuning is a crucial transfer learning technique. The core idea is to leverage a foundational model that has been pre-trained on massive unlabeled corpora and possesses rich world knowledge and language capabilities, then further train it using relatively smaller, labeled datasets targeted at specific downstream tasks or domains.\n",
    "\n",
    "This process doesn't start from scratch; instead, it adjusts the model's existing parameters to better align its knowledge structure and capability distribution with the data characteristics and requirements of the target task. Through fine-tuning, the model transforms from a \"general problem solver\" to a \"domain-specific expert,\" achieving significant performance improvements in target applications.\n",
    "\n",
    "### II. Core Objectives and Application Value\n",
    "\n",
    "The primary goals of model fine-tuning are to achieve model customization, specialization, and alignment.\n",
    "\n",
    "### III. Technology Stack and Methodology Adopted in This Tutorial\n",
    "\n",
    "To ensure efficiency and reproducibility of this fine-tuning practice, this tutorial is built upon a cutting-edge set of open-source tools and advanced methodologies:\n",
    "\n",
    "**Core Optimization Engine: Unsloth**\n",
    "\n",
    "An open-source library focused on improving LLM fine-tuning efficiency. Through its deeply optimized CUDA kernels, this tutorial will achieve up to 2x speed improvement and over 60% memory usage reduction throughout the training process, enabling large-scale model fine-tuning on consumer-grade hardware.\n",
    "\n",
    "**Key Implementation Technologies:**\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning (PEFT / LoRA):** This tutorial will adopt Low-Rank Adaptation (LoRA) technology. This method freezes most of the model's original parameters and only introduces and trains a small number of pluggable adapter modules, significantly reducing computational and storage overhead during training while maintaining model performance.\n",
    "\n",
    "**4-bit Model Quantization:** Using the bitsandbytes library, this tutorial will apply 4-bit quantization technology during model loading. This technique compresses the model's static memory footprint and runtime memory peaks by reducing the numerical precision of model weights, which is key to enabling large model fine-tuning in resource-constrained environments.\n",
    "\n",
    "This tutorial will use the open-source model baidu/ERNIE-4.5-21B-A3B-PT as the experimental base model, with all practices built upon the industry-standard Hugging Face ecosystem (TRL, Transformers). Now, let's formally begin our fine-tuning exploration journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup: Install Unsloth and Related Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version of unsloth from GitHub source code\n",
    "# This ensures we're using the latest features and fixes\n",
    "!pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "\n",
    "# Install bitsandbytes and unsloth_zoo packages\n",
    "# bitsandbytes is a library for quantization and model optimization, helping reduce memory usage\n",
    "# unsloth_zoo contains pre-trained models or other tools for convenient use\n",
    "!pip install bitsandbytes unsloth_zoo\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Model and Configure LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Configure PEFT (LoRA):** Directly fine-tuning the entire large model is impractical due to massive computational resource requirements. Therefore, we adopt a technique called PEFT (Parameter-Efficient Fine-Tuning), with LoRA (Low-Rank Adaptation) being the most classic approach. The FastLanguageModel.get_peft_model function will add LoRA adapters to our model.\n",
    "\n",
    "**Principle:** The core idea of LoRA is to freeze most of the original model's parameters and only inject small, trainable \"adapter\" layers in key parts of the model (such as the attention layers defined in target_modules).\n",
    "\n",
    "**Effect:** Instead of training all tens of billions of parameters, we only train a few million LoRA parameters. This dramatically reduces the memory and time required for training.\n",
    "\n",
    "`use_gradient_checkpointing=True` is another important memory optimization technique that further reduces peak memory usage during training by recomputing intermediate activations during backpropagation rather than storing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load model\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "MAX_LEN = 4096\n",
    "\n",
    "# Two model sizes are available for the workflow below: baidu/ERNIE-4.5-0.3B-PT, baidu/ERNIE-4.5-21B-A3B-PT\n",
    "# For first-time script testing, we recommend using the 0.3B small model for quick experience (though it may struggle with most real downstream tasks)\n",
    "# For normal task training, we recommend switching to A100 GPU and training with the 21B model\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name      = \"baidu/ERNIE-4.5-0.3B-PT\", # baidu/ERNIE-4.5-0.3B-PT, baidu/ERNIE-4.5-21B-A3B-PT\n",
    "    # max_seq_length  = MAX_LEN,\n",
    "    # load_in_4bit    = False,            # Common QLoRA configuration, set to True if memory is tight\n",
    "    # load_in_8bit    = False,\n",
    "    # full_finetuning = False,           # LoRA/QLoRA training; set True for full parameter training (requires more memory)\n",
    "    trust_remote_code = True,          # Required for ERNIE\n",
    ")\n",
    "\n",
    "# 2) Configure LoRA\n",
    "from unsloth import FastLanguageModel\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, lora_alpha = 32, lora_dropout = 0.05,\n",
    "    # The r value can be increased based on task difficulty and data volume (larger rank can store more new knowledge)\n",
    "    # Alpha is recommended to be set to 2x the rank\n",
    "    target_modules = \"all-linear\",     # Unsloth will automatically handle; you can also specify specific modules\n",
    "    use_rslora = True,                 # Common stable configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Fine-tuning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ready, now we need to prepare the \"textbook\" for it - our training dataset. The effectiveness of large model fine-tuning largely depends on the quality and format of the training data. If the data format doesn't match the format used during model pre-training, training effectiveness will be significantly reduced.\n",
    "\n",
    "This code loads a raw dataset and strictly converts it into the conversation format expected by the ERNIE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Prepare data (Example: ShareGPT / chat format)\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"microsoft/orca-math-word-problems-200k\", split=\"train[:1%]\")  # Replace with your data\n",
    "\n",
    "def format_chat(ex):\n",
    "    msgs = [{\"role\": \"user\", \"content\": ex[\"question\"]}]\n",
    "    if \"answer\" in ex and ex[\"answer\"]:\n",
    "        msgs.append({\"role\": \"assistant\",\"content\": ex[\"answer\"]})\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        msgs, tokenize=False, add_generation_prompt=False\n",
    "    )}\n",
    "ds = ds.map(format_chat, remove_columns=ds.column_names)\n",
    "\n",
    "# Print one data sample to see the underlying input sequence for the model\n",
    "ds[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, ERNIE's underlying sequence structure is: `<|begin_of_sentence|>User: XXX.\\nAssistant: XXX<|end_of_sentence|>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Configure and Launch Model Fine-tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll configure the Trainer from the transformers library and launch the fine-tuning process. Unsloth integrates seamlessly with trl's SFTTrainer in the background, allowing us to define complex training loops with very concise code.\n",
    "\n",
    "The workflow in this code block is as follows:\n",
    "\n",
    "**Instantiate SFTTrainer:**\n",
    "- SFTTrainer is a powerful tool specifically designed for Supervised Fine-Tuning. We pass the prepared model, tokenizer, and formatted dataset to it.\n",
    "- `dataset_text_field=\"text\"` explicitly tells the trainer that the column containing the text content we want to train on is called \"text\".\n",
    "- `max_seq_length` ensures that input data will be truncated or padded to our previously set maximum length.\n",
    "\n",
    "**Configure Training Parameters (TrainingArguments):**\n",
    "- This is the \"control panel\" for fine-tuning, where we define all training hyperparameters.\n",
    "- **Memory optimization combination:** `per_device_train_batch_size=2` and `gradient_accumulation_steps=4` work together to achieve an effective batch size of 8 (2 * 4) while keeping single-iteration memory requirements low. This is a key technique for training with limited memory.\n",
    "- **Performance optimization:** Automatically detects and enables bf16 or fp16 mixed precision training, which can significantly improve training speed. `optim=\"adamw_8bit\"` uses 8-bit optimizer states to further save memory.\n",
    "- **Training schedule:** `max_steps=60` means we're only training for 60 steps for quick demonstration. In practice, adjust this to a larger value (e.g., 500-1000 steps) based on task complexity and data volume. `learning_rate=2e-4` is a commonly used learning rate for LoRA fine-tuning.\n",
    "\n",
    "**Start Training (trainer.train()):**\n",
    "- After calling this simple function, the complex training loop (data loading, forward pass, loss calculation, backpropagation, parameter updates) begins automatically.\n",
    "- In the output, you'll see the training loss decreasing continuously, indicating that the model is learning from the data and becoming increasingly \"intelligent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Train with TRL's SFTTrainer (Unsloth officially recommended approach)\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    learning_rate = 1e-4,             # Can be reduced to 2e-5 for longer training\n",
    "    logging_steps = 5,\n",
    "    num_train_epochs = 1,             # Or use max_steps\n",
    "    fp16 = True,\n",
    "    bf16 = False,\n",
    "    optim = \"adamw_8bit\",\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    weight_decay = 0.01,\n",
    "    output_dir = \"outputs\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 200,                 # Save checkpoint every 200 steps for easy resume\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = ds,\n",
    "    dataset_text_field = \"text\",\n",
    "    args = args,\n",
    "    max_seq_length = MAX_LEN,\n",
    "    packing = True,                   # Makes sample concatenation more efficient\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()       # Can pass resume_from_checkpoint=True to resume training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save the Fine-tuned LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\n",
    "    \"ernie-4.5-0.3b-sft-merged\",\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved post-training weights are located at: /content/ernie-4.5-0.3b-sft-merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory if needed\n",
    "# import gc\n",
    "# del model, tokenizer, trainer, dpo_trainer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}